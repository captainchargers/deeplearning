{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP XOR Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement tensorflow code for XOR operation using Multi Layer Perceptron (a.k.a MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single perceptron only works on linearly separable classification\n",
    "One perceptron is one decision boundary, so it only solve linearly separable problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Perceptron](https://qph.fs.quoracdn.net/main-qimg-a6c557af4280d1f85cacc66e048e82f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP (multi layer perceptron) with two neurons in hidden layer can solve XOR.  \n",
    "Two neurons in hidden layer will draw two boundary lines (z1, z2), \n",
    "\n",
    "we can make z1, z2 truth table like below,\n",
    "z1, z2, value\n",
    "0,  0,  0\n",
    "0,  1,  1\n",
    "1,  0,  1\n",
    "\n",
    "As you can see from below upper 2d chart, now it is linearly separable on z1, z2 axis,  \n",
    "one perceptron in the next layer can classify output from hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Perceptron](http://cps0715.weebly.com/uploads/7/4/0/3/74035485/8009014_orig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because step function is hard to optimize using back propagation due to Non-differentiable,  \n",
    "We will use sigmoid as its activation instead of step function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Perceptron](https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/MLP_XOR.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tensorflow Graph\n",
    "firstly, we will define train data shape.  \n",
    "XOR train data has input X and output Y.  \n",
    "\n",
    "X is [4,2] shape like below,  \n",
    "[0, 0], [0, 1], [1, 0], [1, 1]  \n",
    "\n",
    "Y is [4,1] shape like below,  \n",
    "[[0], [1], [1], [0]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[4,2])\n",
    "Y = tf.placeholder(tf.float32, shape=[4,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define first layer has two neurons taking two input values.  \n",
    "W1 = tf.Variable(tf.random_uniform([2,2]))\n",
    "# each neuron has one bias.\n",
    "B1 = tf.Variable(tf.zeros([2]))\n",
    "# First Layer's output is Z which is the sigmoid(W1 * X + B1)\n",
    "Z = tf.sigmoid(tf.matmul(X, W1) + B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define second layer has one neurons taking two input values.  \n",
    "W2 = tf.Variable(tf.random_uniform([2,1]))\n",
    "# one neuron has one bias.\n",
    "B2 = tf.Variable(tf.zeros([1]))\n",
    "# Second Layer's output is Y_hat which is the sigmoid(W2 * Z + B2)\n",
    "Y_hat = tf.sigmoid(tf.matmul(Z, W2) + B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy\n",
    "loss = tf.reduce_mean(-1*((Y*tf.log(Y_hat))+((1-Y)*tf.log(1.0-Y_hat))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "train_Y = [[0],[1],[1],[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
      "Epoch :  0\n",
      "Output :  [[0.5759099]\n",
      " [0.5988261]\n",
      " [0.5978792]\n",
      " [0.617011 ]]\n",
      "Epoch :  5000\n",
      "Output :  [[0.24778686]\n",
      " [0.63629276]\n",
      " [0.55345565]\n",
      " [0.59068745]]\n",
      "Epoch :  10000\n",
      "Output :  [[0.075223  ]\n",
      " [0.9185553 ]\n",
      " [0.91934305]\n",
      " [0.10548425]]\n",
      "Epoch :  15000\n",
      "Output :  [[0.0316886 ]\n",
      " [0.97331434]\n",
      " [0.9735103 ]\n",
      " [0.03062824]]\n",
      "Final Output :  [[0.01958315]\n",
      " [0.9845865 ]\n",
      " [0.98467547]\n",
      " [0.01702813]]\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    print(\"train data: \"+str(train_X))\n",
    "    for i in range(20000):\n",
    "        sess.run(train_step, feed_dict={X: train_X, Y: train_Y})\n",
    "        if i % 5000 == 0:\n",
    "            print('Epoch : ', i)\n",
    "            print('Output : ', sess.run(Y_hat, feed_dict={X: train_X, Y: train_Y}))\n",
    "    \n",
    "    print('Final Output : ', sess.run(Y_hat, feed_dict={X: train_X, Y: train_Y}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
