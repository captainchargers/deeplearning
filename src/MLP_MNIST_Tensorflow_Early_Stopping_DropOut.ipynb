{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP (MNIST, Tensorflow) with Early Stopping, DropOut\n",
    "In this tutorial, we will apply early stopping, dropout on MNIST MLP tensorflow code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Architecture\n",
    "here is the overview of MLP architecture we will implement with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/dropout.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/dropout.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data has **60000** samples  \n",
    "test data has **10000** samples   \n",
    "every data is **28 * 28** pixels  \n",
    "\n",
    "below image shows 28*28 pixel image sample for hand written number '0' from MNIST data.  \n",
    "MNIST is gray scale image [0 to 255] for hand written number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![0 from MNIST](https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/mnist_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train data into train and validation data\n",
    "Validation during training gives advantages below,  \n",
    "1) check if train goes well based on validation score  \n",
    "2) apply **early stopping** when validation score doesn't improve while train score goes up (overcome **overfitting**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val  = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val  = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 50000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"train data has \" + str(x_train.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_train.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation data has 10000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"validation data has \" + str(x_val.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_val.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28 * 28 pixels has gray scale value from **0** to **255**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# sample to show gray scale values\n",
    "print(x_train[0][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each train data has its label **0** to **9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1]\n"
     ]
    }
   ],
   "source": [
    "# sample to show labels for first train data to 10th train data\n",
    "print(y_train[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data has **10000** samples  \n",
    "every test data is **28 * 28** image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data has 10000 samples\n",
      "every test data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"test data has \" + str(x_test.shape[0]) + \" samples\")\n",
    "print(\"every test data is \" + str(x_test.shape[1]) \n",
    "      + \" * \" + str(x_test.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape\n",
    "In order to fully connect all pixels to hidden layer,  \n",
    "we will reshape (28, 28) into (28x28,1) shape.  \n",
    "It means we flatten row x column shape to an array having 28x28 (756) items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(50000, 784)\n",
    "x_val = x_val.reshape(10000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize data\n",
    "normalization usually helps faster learning speed, better performance  \n",
    "by reducing variance and giving same range to all input features.  \n",
    "since MNIST data set all input has 0 to 255, normalization only helps reducing variances.  \n",
    "it turned out normalization is better than standardization for MNIST data with my MLP architeture,    \n",
    "I believe this is because relu handles 0 differently on both feed forward and back propagation.  \n",
    "handling 0 differently is important for MNIST, since 1-255 means there is some hand written,  \n",
    "while 0 means no hand written on that pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_val /= gray_scale\n",
    "x_test /= gray_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label to one hot encoding value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow MLP Graph\n",
    "Let's implement the MLP graph with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Out\n",
    "We will apply dropout at the last hidden layer.  \n",
    "Dropout is a avoiding overfitting technic by randomly deactivate some nodes in the hidden layer.  \n",
    "Dropout has ensemble effect since every mini batch has different activated nodes.  \n",
    "Adjusting variables in the deep learning model with drop out mini batch result is similar to random forest approach.  \n",
    "keep_prob is the tensorflow placeholder for dropout ratio.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the hidden layer2, you can see we use dropout.  \n",
    "keep_prob will be filled when we train or test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x):\n",
    "    # hidden layer1\n",
    "    w1 = tf.Variable(tf.random_uniform([784,256]))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    # hidden layer2\n",
    "    w2 = tf.Variable(tf.random_uniform([256,128]))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "    h2_drop = tf.nn.dropout(h2, keep_prob)\n",
    "    # output layer\n",
    "    w3 = tf.Variable(tf.random_uniform([128,10]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    logits= tf.matmul(h2_drop, w3) + b3\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping\n",
    "When validation accuracy doesn't improve while train accuracy keep improves,  \n",
    "we can early stop the train in order to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/early_stop.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/early_stop.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# train hyperparameters\n",
    "epoch_cnt = 300\n",
    "batch_size = 1000\n",
    "iteration = len(x_train) // batch_size\n",
    "\n",
    "earlystop_threshold = 5\n",
    "earlystop_cnt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we feed data, we must make sure to fill keep_prob for drop out ratio.  \n",
    "I used 0.9 for train, since I want to dropout 10% of nodes in the hidden layer2.  \n",
    "I used 1.0 for test, since I don't want to dropout during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train acc: 0.14934, val acc: 0.1518\n",
      "epoch: 1, train acc: 0.5617, val acc: 0.585\n",
      "epoch: 2, train acc: 0.57466, val acc: 0.5973\n",
      "epoch: 3, train acc: 0.62728, val acc: 0.6573\n",
      "epoch: 4, train acc: 0.66994, val acc: 0.6933\n",
      "epoch: 5, train acc: 0.71234, val acc: 0.734\n",
      "epoch: 6, train acc: 0.74974, val acc: 0.7668\n",
      "epoch: 7, train acc: 0.77604, val acc: 0.7897\n",
      "epoch: 8, train acc: 0.79734, val acc: 0.8072\n",
      "epoch: 9, train acc: 0.81464, val acc: 0.8209\n",
      "epoch: 10, train acc: 0.82668, val acc: 0.8333\n",
      "epoch: 11, train acc: 0.83912, val acc: 0.8436\n",
      "epoch: 12, train acc: 0.84832, val acc: 0.852\n",
      "epoch: 13, train acc: 0.8569, val acc: 0.8591\n",
      "epoch: 14, train acc: 0.86394, val acc: 0.8664\n",
      "epoch: 15, train acc: 0.87184, val acc: 0.8728\n",
      "epoch: 16, train acc: 0.87796, val acc: 0.878\n",
      "epoch: 17, train acc: 0.883, val acc: 0.8847\n",
      "epoch: 18, train acc: 0.88768, val acc: 0.8886\n",
      "epoch: 19, train acc: 0.89228, val acc: 0.894\n",
      "epoch: 20, train acc: 0.89612, val acc: 0.8959\n",
      "epoch: 21, train acc: 0.9003, val acc: 0.8985\n",
      "epoch: 22, train acc: 0.90356, val acc: 0.9004\n",
      "epoch: 23, train acc: 0.90676, val acc: 0.9027\n",
      "epoch: 24, train acc: 0.90954, val acc: 0.905\n",
      "epoch: 25, train acc: 0.91176, val acc: 0.9063\n",
      "epoch: 26, train acc: 0.91406, val acc: 0.9101\n",
      "epoch: 27, train acc: 0.9165, val acc: 0.9109\n",
      "epoch: 28, train acc: 0.91912, val acc: 0.9118\n",
      "epoch: 29, train acc: 0.92116, val acc: 0.9139\n",
      "epoch: 30, train acc: 0.92358, val acc: 0.9167\n",
      "epoch: 31, train acc: 0.92558, val acc: 0.9188\n",
      "epoch: 32, train acc: 0.92692, val acc: 0.9181\n",
      "overfitting warning: 0\n",
      "epoch: 33, train acc: 0.92858, val acc: 0.92\n",
      "epoch: 34, train acc: 0.93106, val acc: 0.9206\n",
      "epoch: 35, train acc: 0.93248, val acc: 0.9217\n",
      "epoch: 36, train acc: 0.9345, val acc: 0.923\n",
      "epoch: 37, train acc: 0.93538, val acc: 0.9227\n",
      "overfitting warning: 0\n",
      "epoch: 38, train acc: 0.93724, val acc: 0.9237\n",
      "epoch: 39, train acc: 0.93872, val acc: 0.9241\n",
      "epoch: 40, train acc: 0.93954, val acc: 0.9261\n",
      "epoch: 41, train acc: 0.9408, val acc: 0.9264\n",
      "epoch: 42, train acc: 0.94226, val acc: 0.9278\n",
      "epoch: 43, train acc: 0.94298, val acc: 0.9289\n",
      "epoch: 44, train acc: 0.94458, val acc: 0.93\n",
      "epoch: 45, train acc: 0.94582, val acc: 0.9312\n",
      "epoch: 46, train acc: 0.94666, val acc: 0.9313\n",
      "epoch: 47, train acc: 0.94762, val acc: 0.9314\n",
      "epoch: 48, train acc: 0.9493, val acc: 0.932\n",
      "epoch: 49, train acc: 0.95, val acc: 0.9324\n",
      "epoch: 50, train acc: 0.95104, val acc: 0.9323\n",
      "overfitting warning: 0\n",
      "epoch: 51, train acc: 0.95226, val acc: 0.9331\n",
      "epoch: 52, train acc: 0.9532, val acc: 0.9328\n",
      "overfitting warning: 0\n",
      "epoch: 53, train acc: 0.95378, val acc: 0.9341\n",
      "epoch: 54, train acc: 0.95542, val acc: 0.9353\n",
      "epoch: 55, train acc: 0.95566, val acc: 0.9344\n",
      "overfitting warning: 0\n",
      "epoch: 56, train acc: 0.95618, val acc: 0.9339\n",
      "overfitting warning: 1\n",
      "epoch: 57, train acc: 0.95778, val acc: 0.9354\n",
      "epoch: 58, train acc: 0.95866, val acc: 0.9356\n",
      "epoch: 59, train acc: 0.9585, val acc: 0.9351\n",
      "epoch: 60, train acc: 0.95996, val acc: 0.9357\n",
      "epoch: 61, train acc: 0.96062, val acc: 0.9358\n",
      "epoch: 62, train acc: 0.96002, val acc: 0.9349\n",
      "epoch: 63, train acc: 0.96178, val acc: 0.9356\n",
      "overfitting warning: 0\n",
      "epoch: 64, train acc: 0.96248, val acc: 0.9379\n",
      "epoch: 65, train acc: 0.96358, val acc: 0.9379\n",
      "epoch: 66, train acc: 0.9637, val acc: 0.9373\n",
      "overfitting warning: 0\n",
      "epoch: 67, train acc: 0.96504, val acc: 0.9374\n",
      "overfitting warning: 1\n",
      "epoch: 68, train acc: 0.96534, val acc: 0.9375\n",
      "overfitting warning: 2\n",
      "epoch: 69, train acc: 0.9657, val acc: 0.9378\n",
      "overfitting warning: 3\n",
      "epoch: 70, train acc: 0.96624, val acc: 0.9391\n",
      "epoch: 71, train acc: 0.9666, val acc: 0.9397\n",
      "epoch: 72, train acc: 0.968, val acc: 0.94\n",
      "epoch: 73, train acc: 0.96786, val acc: 0.9402\n",
      "epoch: 74, train acc: 0.96946, val acc: 0.941\n",
      "epoch: 75, train acc: 0.96952, val acc: 0.9414\n",
      "epoch: 76, train acc: 0.96924, val acc: 0.9401\n",
      "epoch: 77, train acc: 0.9698, val acc: 0.9397\n",
      "overfitting warning: 0\n",
      "epoch: 78, train acc: 0.9707, val acc: 0.9428\n",
      "epoch: 79, train acc: 0.9711, val acc: 0.9423\n",
      "overfitting warning: 0\n",
      "epoch: 80, train acc: 0.9727, val acc: 0.9431\n",
      "epoch: 81, train acc: 0.97234, val acc: 0.9434\n",
      "epoch: 82, train acc: 0.97266, val acc: 0.9425\n",
      "overfitting warning: 0\n",
      "epoch: 83, train acc: 0.97346, val acc: 0.9432\n",
      "overfitting warning: 1\n",
      "epoch: 84, train acc: 0.97446, val acc: 0.9438\n",
      "epoch: 85, train acc: 0.97454, val acc: 0.9428\n",
      "overfitting warning: 0\n",
      "epoch: 86, train acc: 0.97474, val acc: 0.9445\n",
      "epoch: 87, train acc: 0.97552, val acc: 0.945\n",
      "epoch: 88, train acc: 0.97578, val acc: 0.9431\n",
      "overfitting warning: 0\n",
      "epoch: 89, train acc: 0.9757, val acc: 0.9444\n",
      "epoch: 90, train acc: 0.9769, val acc: 0.9434\n",
      "overfitting warning: 0\n",
      "epoch: 91, train acc: 0.97744, val acc: 0.945\n",
      "epoch: 92, train acc: 0.9787, val acc: 0.9455\n",
      "epoch: 93, train acc: 0.97788, val acc: 0.9446\n",
      "epoch: 94, train acc: 0.97854, val acc: 0.9453\n",
      "overfitting warning: 0\n",
      "epoch: 95, train acc: 0.9785, val acc: 0.9457\n",
      "epoch: 96, train acc: 0.97986, val acc: 0.946\n",
      "epoch: 97, train acc: 0.97984, val acc: 0.946\n",
      "epoch: 98, train acc: 0.98062, val acc: 0.9462\n",
      "epoch: 99, train acc: 0.98024, val acc: 0.9469\n",
      "epoch: 100, train acc: 0.98068, val acc: 0.9471\n",
      "epoch: 101, train acc: 0.98176, val acc: 0.947\n",
      "overfitting warning: 0\n",
      "epoch: 102, train acc: 0.9814, val acc: 0.9471\n",
      "epoch: 103, train acc: 0.98242, val acc: 0.9487\n",
      "epoch: 104, train acc: 0.9824, val acc: 0.9471\n",
      "epoch: 105, train acc: 0.9818, val acc: 0.9485\n",
      "epoch: 106, train acc: 0.98302, val acc: 0.95\n",
      "epoch: 107, train acc: 0.98298, val acc: 0.9488\n",
      "epoch: 108, train acc: 0.98378, val acc: 0.9498\n",
      "overfitting warning: 0\n",
      "epoch: 109, train acc: 0.98264, val acc: 0.9482\n",
      "epoch: 110, train acc: 0.98354, val acc: 0.9485\n",
      "overfitting warning: 0\n",
      "epoch: 111, train acc: 0.98332, val acc: 0.9484\n",
      "epoch: 112, train acc: 0.98318, val acc: 0.9496\n",
      "epoch: 113, train acc: 0.98458, val acc: 0.9501\n",
      "epoch: 114, train acc: 0.98398, val acc: 0.9491\n",
      "epoch: 115, train acc: 0.9848, val acc: 0.9495\n",
      "overfitting warning: 0\n",
      "epoch: 116, train acc: 0.98544, val acc: 0.9491\n",
      "overfitting warning: 1\n",
      "epoch: 117, train acc: 0.98564, val acc: 0.9512\n",
      "epoch: 118, train acc: 0.98624, val acc: 0.9516\n",
      "epoch: 119, train acc: 0.98528, val acc: 0.9506\n",
      "epoch: 120, train acc: 0.98508, val acc: 0.9496\n",
      "epoch: 121, train acc: 0.9842, val acc: 0.9488\n",
      "epoch: 122, train acc: 0.98288, val acc: 0.9473\n",
      "epoch: 123, train acc: 0.98246, val acc: 0.948\n",
      "epoch: 124, train acc: 0.98518, val acc: 0.9516\n",
      "epoch: 125, train acc: 0.9846, val acc: 0.9498\n",
      "epoch: 126, train acc: 0.98614, val acc: 0.9497\n",
      "overfitting warning: 0\n",
      "epoch: 127, train acc: 0.98482, val acc: 0.9508\n",
      "epoch: 128, train acc: 0.98524, val acc: 0.9498\n",
      "overfitting warning: 0\n",
      "epoch: 129, train acc: 0.9863, val acc: 0.9501\n",
      "overfitting warning: 1\n",
      "epoch: 130, train acc: 0.98562, val acc: 0.9513\n",
      "epoch: 131, train acc: 0.98658, val acc: 0.9508\n",
      "overfitting warning: 0\n",
      "epoch: 132, train acc: 0.98414, val acc: 0.9493\n",
      "epoch: 133, train acc: 0.9857, val acc: 0.9487\n",
      "overfitting warning: 0\n",
      "epoch: 134, train acc: 0.9867, val acc: 0.9501\n",
      "overfitting warning: 1\n",
      "epoch: 135, train acc: 0.98732, val acc: 0.9519\n",
      "epoch: 136, train acc: 0.98798, val acc: 0.9506\n",
      "overfitting warning: 0\n",
      "epoch: 137, train acc: 0.98624, val acc: 0.9491\n",
      "epoch: 138, train acc: 0.987, val acc: 0.9527\n",
      "epoch: 139, train acc: 0.9863, val acc: 0.9523\n",
      "epoch: 140, train acc: 0.98806, val acc: 0.9548\n",
      "epoch: 141, train acc: 0.98792, val acc: 0.9526\n",
      "epoch: 142, train acc: 0.98784, val acc: 0.9519\n",
      "epoch: 143, train acc: 0.98722, val acc: 0.9518\n",
      "epoch: 144, train acc: 0.9884, val acc: 0.9514\n",
      "overfitting warning: 0\n",
      "epoch: 145, train acc: 0.98562, val acc: 0.952\n",
      "epoch: 146, train acc: 0.9823, val acc: 0.9485\n",
      "epoch: 147, train acc: 0.98936, val acc: 0.9536\n",
      "overfitting warning: 0\n",
      "epoch: 148, train acc: 0.98716, val acc: 0.953\n",
      "epoch: 149, train acc: 0.98628, val acc: 0.9525\n",
      "epoch: 150, train acc: 0.98576, val acc: 0.9521\n",
      "epoch: 151, train acc: 0.98664, val acc: 0.9494\n",
      "overfitting warning: 0\n",
      "epoch: 152, train acc: 0.9884, val acc: 0.9552\n",
      "epoch: 153, train acc: 0.9892, val acc: 0.9531\n",
      "overfitting warning: 0\n",
      "epoch: 154, train acc: 0.98852, val acc: 0.9528\n",
      "epoch: 155, train acc: 0.98786, val acc: 0.9529\n",
      "epoch: 156, train acc: 0.98874, val acc: 0.9538\n",
      "overfitting warning: 0\n",
      "epoch: 157, train acc: 0.98928, val acc: 0.9549\n",
      "overfitting warning: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 158, train acc: 0.99028, val acc: 0.9553\n",
      "epoch: 159, train acc: 0.99028, val acc: 0.9561\n",
      "epoch: 160, train acc: 0.99018, val acc: 0.9562\n",
      "epoch: 161, train acc: 0.9893, val acc: 0.9547\n",
      "epoch: 162, train acc: 0.98958, val acc: 0.9554\n",
      "overfitting warning: 0\n",
      "epoch: 163, train acc: 0.98898, val acc: 0.9534\n",
      "epoch: 164, train acc: 0.98744, val acc: 0.9508\n",
      "epoch: 165, train acc: 0.9909, val acc: 0.9559\n",
      "overfitting warning: 0\n",
      "epoch: 166, train acc: 0.98936, val acc: 0.9542\n",
      "epoch: 167, train acc: 0.98948, val acc: 0.9559\n",
      "overfitting warning: 0\n",
      "epoch: 168, train acc: 0.98962, val acc: 0.9566\n",
      "epoch: 169, train acc: 0.99068, val acc: 0.9553\n",
      "overfitting warning: 0\n",
      "epoch: 170, train acc: 0.98922, val acc: 0.953\n",
      "epoch: 171, train acc: 0.99092, val acc: 0.9581\n",
      "epoch: 172, train acc: 0.9866, val acc: 0.9522\n",
      "epoch: 173, train acc: 0.99192, val acc: 0.9564\n",
      "overfitting warning: 0\n",
      "epoch: 174, train acc: 0.99118, val acc: 0.9571\n",
      "overfitting warning: 1\n",
      "epoch: 175, train acc: 0.98826, val acc: 0.9536\n",
      "epoch: 176, train acc: 0.99054, val acc: 0.956\n",
      "overfitting warning: 0\n",
      "epoch: 177, train acc: 0.99044, val acc: 0.9565\n",
      "overfitting warning: 1\n",
      "epoch: 178, train acc: 0.99214, val acc: 0.9577\n",
      "overfitting warning: 2\n",
      "epoch: 179, train acc: 0.99076, val acc: 0.9561\n",
      "overfitting warning: 3\n",
      "epoch: 180, train acc: 0.99002, val acc: 0.9553\n",
      "overfitting warning: 4\n",
      "epoch: 181, train acc: 0.99154, val acc: 0.9589\n",
      "epoch: 182, train acc: 0.98936, val acc: 0.9544\n",
      "epoch: 183, train acc: 0.9904, val acc: 0.9573\n",
      "overfitting warning: 0\n",
      "epoch: 184, train acc: 0.98904, val acc: 0.9556\n",
      "epoch: 185, train acc: 0.98958, val acc: 0.9555\n",
      "overfitting warning: 0\n",
      "epoch: 186, train acc: 0.99036, val acc: 0.9553\n",
      "overfitting warning: 1\n",
      "epoch: 187, train acc: 0.99202, val acc: 0.9578\n",
      "overfitting warning: 2\n",
      "epoch: 188, train acc: 0.99204, val acc: 0.9579\n",
      "overfitting warning: 3\n",
      "epoch: 189, train acc: 0.99112, val acc: 0.9573\n",
      "overfitting warning: 4\n",
      "epoch: 190, train acc: 0.99126, val acc: 0.9562\n",
      "early stopped on 190\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    prev_train_acc = 0.0\n",
    "    max_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.\n",
    "        start = 0; end = batch_size\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            _, loss = sess.run([train_op, loss_op], \n",
    "                               feed_dict={x: x_train[start: end], y: y_train[start: end], \n",
    "                                          keep_prob: 0.9})\n",
    "            start += batch_size; end += batch_size\n",
    "            # Compute train average loss\n",
    "            avg_loss += loss / iteration\n",
    "            \n",
    "        # Validate model\n",
    "        preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        # train accuracy\n",
    "        cur_train_acc = accuracy.eval({x: x_train, y: y_train,keep_prob: 1.0})\n",
    "        # validation accuarcy\n",
    "        cur_val_acc = accuracy.eval({x: x_val, y: y_val, keep_prob: 1.0})\n",
    "        # validation loss\n",
    "        cur_val_loss = loss_op.eval({x: x_val, y: y_val,keep_prob: 1.0})\n",
    "        \n",
    "        print(\"epoch: \"+str(epoch)+\n",
    "              \", train acc: \" + str(cur_train_acc) +\n",
    "              \", val acc: \" + str(cur_val_acc) )\n",
    "              #', train loss: '+str(avg_loss)+\n",
    "              #', val loss: '+str(cur_val_loss))\n",
    "        \n",
    "        if cur_val_acc < max_val_acc:\n",
    "            if cur_train_acc > prev_train_acc or cur_train_acc > 0.99:\n",
    "                if earlystop_cnt == earlystop_threshold:\n",
    "                    print(\"early stopped on \"+str(epoch))\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"overfitting warning: \"+str(earlystop_cnt))\n",
    "                    earlystop_cnt += 1\n",
    "            else:\n",
    "                earlystop_cnt = 0\n",
    "        else:\n",
    "            earlystop_cnt = 0\n",
    "            max_val_acc = cur_val_acc\n",
    "            # Save the variables to file.\n",
    "            save_path = saver.save(sess, \"model/model.ckpt\")\n",
    "        prev_train_acc = cur_train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with the best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "[Test Accuracy] : 0.9554\n"
     ]
    }
   ],
   "source": [
    "# Start testing\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"model/model.ckpt\")\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"[Test Accuracy] :\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
